{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Monitoring Locations Field Length Analyzer\n",
        "\n",
        "## Description\n",
        "\n",
        "This script was created using the Google Colab Jupyter Notebook web service\n",
        "\n",
        "This script downloads all monitoring location records from the USGS API endpoint and analyzes the maximum string length of each field across the entire dataset. The purpose is to determine appropriate maximum lengths for each database field to inform schema design and optimize storage.\n",
        "\n",
        "## How it works\n",
        "1. **Data Download:**\n",
        "   - The script uses the USGS API endpoint `https://api.waterdata.usgs.gov/ogcapi/v0/collections/monitoring-locations/items` to retrieve monitoring location data.\n",
        "   - It downloads data in chunks (pages) of up to 1000 records at a time using `limit` and `offset` parameters.\n",
        "   - Pagination continues until all available records (about 41,000) are retrieved or an HTTP error occurs indicating no more data.\n",
        "\n",
        "2. **Data Extraction:**\n",
        "   - From each downloaded feature, the script extracts the `properties` dictionary, which contains all the fields for that record.\n",
        "\n",
        "3. **Data Analysis:**\n",
        "   - Converts the list of properties dictionaries into a Pandas DataFrame.\n",
        "   - For each field (column), calculates the maximum string length found across all records.\n",
        "   - Prints out the maximum lengths in descending order.\n",
        "\n",
        "## Usage\n",
        "- Run the script in an environment with Python and required packages (`requests`, `pandas`) installed.\n",
        "- The resulting max field lengths help in setting precise string length limits for database schema fields, improving data integrity and storage efficiency.\n",
        "\n",
        "## Notes\n",
        "- The lengths reflect the current dataset snapshot and may not account for future data variations.\n",
        "- Adding a small buffer to max lengths is recommended for schema design.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB9ED014pXPi",
        "outputId": "e8653649-345d-40d9-d583-88547ad65f3b"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "import json\n",
        "\n",
        "BASE_URL = \"https://api.waterdata.usgs.gov/ogcapi/v0/collections/monitoring-locations/items\"\n",
        "LIMIT = 1000\n",
        "offset = 0\n",
        "all_features = []\n",
        "\n",
        "while True:\n",
        "    params = {\n",
        "        \"limit\": LIMIT,\n",
        "        \"offset\": offset,\n",
        "        \"f\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(BASE_URL, params=params)\n",
        "        response.raise_for_status()\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"HTTP error at offset {offset}: {e}\")\n",
        "        break\n",
        "\n",
        "    data = response.json()\n",
        "    features = data.get(\"features\", [])\n",
        "\n",
        "    if offset == 0:\n",
        "        total_matched = data.get(\"numberMatched\") or data.get(\"totalFeatures\")\n",
        "        if total_matched:\n",
        "            print(f\"Total features reported by API: {total_matched}\")\n",
        "\n",
        "    if not features:\n",
        "        print(\"No more features returned, stopping.\")\n",
        "        break\n",
        "\n",
        "    all_features.extend(features)\n",
        "    print(f\"Downloaded {len(features)} features, total so far: {len(all_features)}\")\n",
        "\n",
        "    offset += LIMIT\n",
        "\n",
        "    # Stop if we've downloaded all or more than total reported (if known)\n",
        "    if total_matched and offset >= total_matched:\n",
        "        print(\"Reached total reported count; stopping.\")\n",
        "        break\n",
        "\n",
        "    time.sleep(0.1)\n",
        "\n",
        "print(f\"Total features downloaded: {len(all_features)}\")\n",
        "\n",
        "with open(\"monitoring_locations_all.json\", \"w\") as f:\n",
        "    json.dump(all_features, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHRQgdoHrBPv",
        "outputId": "9044993a-6007-4e08-ee17-3f74df3e7a3f"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load saved data\n",
        "with open(\"monitoring_locations_all.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract 'properties' dicts from each feature\n",
        "properties_list = [feature['properties'] for feature in data]\n",
        "\n",
        "# Convert to DataFrame for easy analysis\n",
        "df = pd.DataFrame(properties_list)\n",
        "\n",
        "# Calculate max string length per field\n",
        "max_lengths = {}\n",
        "\n",
        "for col in df.columns:\n",
        "    # Convert all to string, then calculate length\n",
        "    max_len = df[col].astype(str).map(len).max()\n",
        "    max_lengths[col] = max_len\n",
        "\n",
        "# Show the max length per field, sorted by length descending\n",
        "max_lengths_sorted = dict(sorted(max_lengths.items(), key=lambda item: item[1], reverse=True))\n",
        "for field, length in max_lengths_sorted.items():\n",
        "    print(f\"{field}: {length}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
